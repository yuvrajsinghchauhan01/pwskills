{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad8d2019",
   "metadata": {},
   "source": [
    "**Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8f873",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale numeric features to a specific range. It transforms the values of the features to a common scale, typically between 0 and 1, based on the minimum and maximum values of the feature.\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "\n",
    "`X_scaled = (X - X_min) / (X_max - X_min)`\n",
    "\n",
    "where X is the original value, X_min is the minimum value of the feature, and X_max is the maximum value of the feature.\n",
    "\n",
    "Min-Max scaling is useful when the features have different scales and ranges, and we want to bring them to a common scale. It helps in preventing features with larger values from dominating the model's learning process.\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Let's say we have a dataset with a feature \"Age\" ranging from 20 to 60 and a feature \"Income\" ranging from 30,000 to 100,000. We want to scale these features to a range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4c69e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Age = [20, 30, 40, 50, 60]\n",
    "Income =  [30000, 40000, 60000, 80000, 100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f59826",
   "metadata": {},
   "source": [
    "To apply Min-Max scaling, we calculate the minimum and maximum values for each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5aea7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Age_min = 20\n",
    "Age_max = 60\n",
    "\n",
    "Income_min = 30000\n",
    "Income_max = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d1ba07",
   "metadata": {},
   "source": [
    "Then, we use the Min-Max scaling formula to transform the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72df6c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Age_scaled = []\n",
    "Income_scaled = []\n",
    "for age in Age:\n",
    "    Age_s = (age - Age_min) / (Age_max - Age_min)\n",
    "    Age_scaled.append(Age_s)\n",
    "    \n",
    "for income in Income: \n",
    "    Income_s = (income - Income_min) / (Income_max - Income_min)\n",
    "    Income_scaled.append(Income_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "67e9e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.25, 0.5, 0.75, 1.0]\n",
      "[0.0, 0.14285714285714285, 0.42857142857142855, 0.7142857142857143, 1.0]\n"
     ]
    }
   ],
   "source": [
    "#Scaled data\n",
    "print(Age_scaled)\n",
    "print(Income_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6417735c",
   "metadata": {},
   "source": [
    "Now, both the \"Age\" and \"Income\" features are scaled to the range between 0 and 1, allowing them to be on a common scale for further analysis or modeling.\n",
    "\n",
    "Min-Max scaling is a simple and effective technique for normalizing features and ensuring they are on a consistent scale, which can be beneficial for many machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f7924c",
   "metadata": {},
   "source": [
    "**Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b2538d",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as normalization or feature scaling, is a data preprocessing technique that rescales the values of a feature to have a unit norm. It transforms the feature vector to a length of 1 while preserving the direction of the vector.\n",
    "\n",
    "The formula for Unit Vector scaling is:\n",
    "\n",
    "`X_scaled = X / ||X||`\n",
    "\n",
    "where X is the original feature vector, X_scaled is the scaled feature vector, and ||X|| represents the Euclidean norm of the feature vector.\n",
    "\n",
    "Unit Vector scaling is useful when the magnitude of the feature values is not as important as their direction or when dealing with sparse data.\n",
    "\n",
    "Here's an example to illustrate the application of the Unit Vector technique:\n",
    "\n",
    "Let's say we have a dataset with a feature \"Height\" and \"Weight\". We want to scale these features to have a unit norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e415790",
   "metadata": {},
   "outputs": [],
   "source": [
    "Height = [160, 170, 180]\n",
    "Weight = [60, 70, 80]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d986d463",
   "metadata": {},
   "source": [
    "To apply Unit Vector scaling, we calculate the Euclidean norm of each feature vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0d0fa0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Height_norm = np.sqrt(160**2 + 170**2 + 180**2)\n",
    "Weight_norm = np.sqrt(60**2 + 70**2 + 80**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f8f627",
   "metadata": {},
   "source": [
    "Then, we divide each feature vector by its respective norm to obtain the scaled feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "266c1425",
   "metadata": {},
   "outputs": [],
   "source": [
    "Height_scaled = [160/Height_norm, 170/Height_norm, 180/Height_norm]\n",
    "Weight_scaled = [60/Weight_norm, 70/Weight_norm, 80/Weight_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e7564af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5427628252422066, 0.5766855018198446, 0.6106081783974825]\n",
      "[0.4915391523114243, 0.5734623443633283, 0.6553855364152323]\n"
     ]
    }
   ],
   "source": [
    "print(Height_scaled)\n",
    "print(Weight_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de801a0",
   "metadata": {},
   "source": [
    "Now, both the \"Height\" and \"Weight\" feature vectors have a unit norm, meaning their lengths are 1. The direction of the vectors is preserved, but the magnitude is scaled down.\n",
    "\n",
    "Unit Vector scaling is particularly useful when the magnitude of the feature values is not important, and we are more interested in the direction or relative importance of the features. It is commonly used in text classification, document clustering, and other applications where the feature vectors represent word frequencies or term frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275d8fe2",
   "metadata": {},
   "source": [
    "**Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e8e284",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional space while preserving the most important information. It achieves this by identifying the principal components, which are linear combinations of the original features that capture the maximum variance in the data.\n",
    "\n",
    "Here's how PCA works in dimensionality reduction:\n",
    "\n",
    "1. Standardize the data: PCA requires the data to be standardized, meaning each feature should have zero mean and unit variance. This is done to ensure that features with larger scales do not dominate the analysis.\n",
    "\n",
    "2. Compute the covariance matrix: The covariance matrix is computed from the standardized data, which represents the relationships between the features.\n",
    "\n",
    "3. Compute the eigenvectors and eigenvalues: The eigenvectors and eigenvalues are calculated from the covariance matrix. The eigenvectors represent the principal components, and the corresponding eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "4. Select the principal components: The principal components are ranked based on their corresponding eigenvalues. The top-k principal components that explain the most variance are selected.\n",
    "\n",
    "5. Transform the data: The original data is projected onto the selected principal components to obtain the lower-dimensional representation.\n",
    "\n",
    "Here's an example to illustrate the application of PCA in dimensionality reduction:\n",
    "\n",
    "Let's say we have a dataset with three features: \"Height,\" \"Weight,\" and \"Age.\" We want to reduce the dimensionality of the dataset to two dimensions using PCA.\n",
    "\n",
    "Original data:\n",
    "- Height: [160, 170, 180]\n",
    "- Weight: [60, 70, 80]\n",
    "- Age: [25, 30, 35]\n",
    "\n",
    "1. Standardize the data: Standardize each feature by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "2. Compute the covariance matrix: Compute the covariance matrix from the standardized data.\n",
    "\n",
    "3. Compute the eigenvectors and eigenvalues: Calculate the eigenvectors and eigenvalues from the covariance matrix.\n",
    "\n",
    "4. Select the principal components: Rank the eigenvectors based on their corresponding eigenvalues. Select the top two eigenvectors as the principal components.\n",
    "\n",
    "5. Transform the data: Project the original data onto the selected principal components to obtain the lower-dimensional representation.\n",
    "\n",
    "Transformed data:\n",
    "- PC1: [0.7071, 0.0000, -0.7071]\n",
    "\n",
    "- PC2: [-0.4082, 0.8165, -0.4082]\n",
    "\n",
    "The transformed data represents the original dataset in a lower-dimensional space, where each data point is represented by two principal components (PC1 and PC2).\n",
    "\n",
    "PCA helps in reducing the dimensionality of the dataset while retaining the most important information. It is commonly used in various applications, such as image recognition, data visualization, and feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b62985",
   "metadata": {},
   "source": [
    "**Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c4c969",
   "metadata": {},
   "source": [
    "PCA and feature extraction are closely related concepts. In fact, PCA can be used as a feature extraction technique.\n",
    "\n",
    "Feature extraction involves transforming the original features of a dataset into a new set of features that capture the most important information. The goal is to reduce the dimensionality of the data while retaining as much relevant information as possible.\n",
    "\n",
    "PCA can be used for feature extraction by identifying the principal components, which are linear combinations of the original features that capture the maximum variance in the data. These principal components can serve as the new set of features.\n",
    "\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Let's say we have a dataset with five features: \"Height,\" \"Weight,\" \"Age,\" \"Income,\" and \"Education Level.\" We want to extract a smaller set of features that capture the most important information.\n",
    "\n",
    "Original data:\n",
    "- Height: [160, 170, 180]\n",
    "- Weight: [60, 70, 80]\n",
    "- Age: [25, 30, 35]\n",
    "- Income: [50000, 60000, 70000]\n",
    "- Education Level: [1, 2, 3]\n",
    "\n",
    "1. Standardize the data: Standardize each feature by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "2. Compute the covariance matrix: Compute the covariance matrix from the standardized data.\n",
    "\n",
    "3. Compute the eigenvectors and eigenvalues: Calculate the eigenvectors and eigenvalues from the covariance matrix.\n",
    "\n",
    "4. Select the principal components: Rank the eigenvectors based on their corresponding eigenvalues. Select the top-k eigenvectors as the principal components.\n",
    "\n",
    "5. Transform the data: Project the original data onto the selected principal components to obtain the lower-dimensional representation.\n",
    "\n",
    "Transformed data:\n",
    "- PC1: [0.7071, 0.0000, -0.7071]\n",
    "- PC2: [-0.4082, 0.8165, -0.4082]\n",
    "\n",
    "In this example, PCA is used as a feature extraction technique to extract two principal components (PC1 and PC2) from the original dataset. These principal components represent a compressed representation of the original features, capturing the most important information.\n",
    "\n",
    "The transformed data with the principal components can be used as the new set of features for further analysis or modeling. By reducing the dimensionality of the data, PCA helps in simplifying the representation of the dataset while retaining the most relevant information.\n",
    "\n",
    "Feature extraction using PCA can be beneficial in various scenarios, such as reducing computational complexity, removing redundant or irrelevant features, and improving the interpretability of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87283cd1",
   "metadata": {},
   "source": [
    "**Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb6bcf6",
   "metadata": {},
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service, you can use Min-Max scaling. Here's how you can apply Min-Max scaling to the dataset:\n",
    "\n",
    "1. Identify the features: In this case, the features are price, rating, and delivery time.\n",
    "\n",
    "2. Determine the range: Decide on the desired range for the scaled values. For example, you might want to scale the features to a range between 0 and 1.\n",
    "\n",
    "3. Compute the minimum and maximum values: Calculate the minimum and maximum values for each feature in the dataset.\n",
    "\n",
    "4. Apply Min-Max scaling: For each feature, use the Min-Max scaling formula to transform the values to the desired range:\n",
    "\n",
    "`X_scaled = (X - X_min) / (X_max - X_min)`\n",
    "\n",
    "where X is the original value, X_min is the minimum value of the feature, and X_max is the maximum value of the feature.\n",
    "\n",
    "By applying Min-Max scaling, you will transform the values of each feature to a common scale between 0 and 1, based on their original range. This ensures that no single feature dominates the recommendation process due to its larger scale.\n",
    "\n",
    "For example, let's say you have the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0503d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "Price =  [10, 20, 30, 40]\n",
    "Rating =  [3.5, 4.2, 4.8, 3.9]\n",
    "Delivery_Time = [20, 30, 25, 35]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e017f95",
   "metadata": {},
   "source": [
    "To apply Min-Max scaling, you would calculate the minimum and maximum values for each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e1c41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Price_min = 10\n",
    "Price_max = 40\n",
    "\n",
    "Rating_min = 3.5\n",
    "Rating_max = 4.8\n",
    "\n",
    "Delivery_Time_min = 20\n",
    "Delivery_Time_max = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612382ab",
   "metadata": {},
   "source": [
    "Then, you would use the Min-Max scaling formula to transform the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93b6e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "Price_scaled = []\n",
    "Rating_scaled = []\n",
    "Delivery_Time_scaled = []\n",
    "for price in Price: \n",
    "    Price_s = (price - Price_min) / (Price_max - Price_min)\n",
    "    Price_scaled.append(Price_s)\n",
    "    \n",
    "for rating in Rating:\n",
    "    Rating_s = (rating - Rating_min) / (Rating_max - Rating_min)\n",
    "    Rating_scaled.append(Rating_s)\n",
    "    \n",
    "for delivery_Time in Delivery_Time:\n",
    "    Delivery_Time_s = (delivery_Time - Delivery_Time_min) / (Delivery_Time_max - Delivery_Time_min)\n",
    "    Delivery_Time_scaled.append(Delivery_Time_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c42d8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.3333333333333333, 0.6666666666666666, 1.0]\n",
      "[0.0, 0.5384615384615387, 1.0, 0.30769230769230765]\n",
      "[0.0, 0.6666666666666666, 0.3333333333333333, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(Price_scaled)\n",
    "print(Rating_scaled)\n",
    "print(Delivery_Time_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7d322",
   "metadata": {},
   "source": [
    "Now, the features are scaled to a range between 0 and 1, allowing them to be on a common scale for the recommendation system. This ensures that each feature contributes equally to the recommendation process, regardless of their original range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa7863",
   "metadata": {},
   "source": [
    "**Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df1ed8",
   "metadata": {},
   "source": [
    "To reduce the dimensionality of the dataset for building a model to predict stock prices, you can use PCA (Principal Component Analysis). Here's how you can apply PCA to the dataset:\n",
    "\n",
    "1. Identify the features: In this case, the features are the various company financial data and market trends.\n",
    "\n",
    "\n",
    "2. Standardize the data: Before applying PCA, it is important to standardize the data by subtracting the mean and dividing by the standard deviation. This ensures that all features are on the same scale and prevents features with larger variances from dominating the analysis.\n",
    "\n",
    "\n",
    "3. Compute the covariance matrix: Calculate the covariance matrix from the standardized data. The covariance matrix represents the relationships between the features.\n",
    "\n",
    "\n",
    "4. Compute the eigenvectors and eigenvalues: Calculate the eigenvectors and eigenvalues from the covariance matrix. The eigenvectors represent the principal components, and the corresponding eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "\n",
    "5. Select the principal components: Rank the eigenvectors based on their corresponding eigenvalues. Select the top-k eigenvectors as the principal components that explain the most variance in the data. The number of principal components to select depends on the desired level of dimensionality reduction.\n",
    "\n",
    "\n",
    "6. Transform the data: Project the original data onto the selected principal components to obtain the lower-dimensional representation. This is done by multiplying the standardized data by the selected eigenvectors.\n",
    "\n",
    "By applying PCA, you will reduce the dimensionality of the dataset while retaining the most important information. The selected principal components represent a compressed representation of the original features, capturing the maximum variance in the data.\n",
    "\n",
    "Reducing the dimensionality of the dataset using PCA can be beneficial for building a model to predict stock prices. It helps in simplifying the representation of the dataset, removing redundant or less informative features, and improving the model's computational efficiency. However, it is important to note that the interpretability of the model may be reduced as the original features are transformed into the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bfca94",
   "metadata": {},
   "source": [
    "**Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e713c7ed",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling on the dataset [1, 5, 10, 15, 20] and transform the values to a range of -1 to 1, follow these steps:\n",
    "\n",
    "1. Determine the minimum and maximum values in the dataset:\n",
    "- Minimum value (X_min) = 1\n",
    "- Maximum value (X_max) = 20\n",
    "\n",
    "2. Apply the Min-Max scaling formula to each value in the dataset:\n",
    "\n",
    "   `X_scaled = (X - X_min) / (X_max - X_min)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4562a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1, 5, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "638f867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min = 1\n",
    "x_max = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f2667b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = []\n",
    "for x in X:\n",
    "    X_SCALED = (x-x_min) / (x_max-x_min)\n",
    "    X_scaled.append(X_SCALED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9d26871c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.21052631578947367, 0.47368421052631576, 0.7368421052631579, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a722430d",
   "metadata": {},
   "source": [
    "**Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b96c31a",
   "metadata": {},
   "source": [
    "To perform feature extraction using PCA on the dataset [height, weight, age, gender, blood pressure], the number of principal components to retain depends on the desired level of dimensionality reduction and the amount of variance explained by each principal component. Here's how you can determine the number of principal components to retain:\n",
    "\n",
    "1. Standardize the data: Before applying PCA, it is important to standardize the data by subtracting the mean and dividing by the standard deviation. This ensures that all features are on the same scale.\n",
    "\n",
    "2. Compute the covariance matrix: Calculate the covariance matrix from the standardized data. The covariance matrix represents the relationships between the features.\n",
    "\n",
    "3. Compute the eigenvectors and eigenvalues: Calculate the eigenvectors and eigenvalues from the covariance matrix. The eigenvectors represent the principal components, and the corresponding eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "4. Determine the explained variance ratio: Calculate the explained variance ratio for each principal component by dividing its eigenvalue by the sum of all eigenvalues. This ratio represents the proportion of the total variance explained by each principal component.\n",
    "\n",
    "5. Select the number of principal components: Decide on the number of principal components to retain based on the desired level of dimensionality reduction and the cumulative explained variance ratio. A common approach is to choose the number of principal components that explain a significant portion of the total variance, such as 80% or 90%.\n",
    "\n",
    "For example, let's say the PCA analysis yields the following eigenvalues and explained variance ratios for the dataset:\n",
    "\n",
    "Eigenvalues: [3.2, 1.8, 1.5, 0.9, 0.6]\n",
    "Explained Variance Ratios: [0.40, 0.22, 0.18, 0.11, 0.09]\n",
    "\n",
    "To determine the number of principal components to retain, you can calculate the cumulative explained variance ratio:\n",
    "\n",
    "Cumulative Explained Variance Ratios: [0.40, 0.62, 0.80, 0.91, 1.00]\n",
    "\n",
    "In this example, the cumulative explained variance ratio reaches 80% after considering the first three principal components. Therefore, you could choose to retain three principal components to capture a significant portion of the total variance in the dataset.\n",
    "\n",
    "The decision of how many principal components to retain ultimately depends on the specific requirements of your project, such as the desired level of dimensionality reduction and the trade-off between simplicity and information loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72843a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
